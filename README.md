# OLEA AI - Intelligent Insurance Advisor üöÄ

Bienvenue dans le d√©p√¥t officiel de la solution **OLEA AI**, con√ßue sp√©cifiquement pour la **Phase II du Hackathon DataQuest OLEA**. Ce projet repr√©sente une architecture MLOps compl√®te, pr√™te pour la production, int√©grant un mod√®le de Machine Learning haute performance, une s√©curit√© Zero-Trust et une exp√©rience conversationnelle GenAI exclusive.

## üåü Vision & Valeur Ajout√©e (Bonus GenAI)
Plut√¥t que de simplement renvoyer une pr√©diction math√©matique brute (ex: "Pack 3") via un formulaire, notre solution int√®gre **NVIDIA LLaMA-70B** comme un Agent Commercial Actif (*Imani*).
Cet agent va de mani√®re proactive :
1. D√©marrer la conversation en demandant au profil client les m√©triques exactes n√©cessaires √† la pr√©diction ML.
2. D√©clencher le mod√®le ML pr√©-entrain√© de la Phase I.
3. Transformer la pr√©diction math√©matique brute en un **argumentaire commercial persuasif, enti√®rement sur-mesure et r√©dig√© en Tounsi (dialecte tunisien)**.
4. **R√©server un rendez-vous automatiquement (Anti-Bureaucratie) :** Si le client est convaincu par l'argumentaire et accepte, l'agent confirme qu'un formulaire de r√©servation vient d'√™tre soumis directement √† OLEA. 

L'√©poque des formulaires ML froids et abstraits est r√©volue : bienvenue dans le courtage conversationnel de demain.

---

## üèóÔ∏è Architecture MLOps & D√©ploiement

Notre syst√®me est divis√© en microservices (API Backend + UI Frontend) pour garantir scalabilit√©, modularit√© et isolation. 

### 1. API d'Inf√©rence S√©curis√©e (FastAPI) `security_api.py`
Le moteur s√©curis√© de notre application :
*   **Validation Stricte (Pydantic) :** Les donn√©es entrantes sont syst√©matiquement filtr√©es pour emp√™cher les injections SQL.
*   **S√©curit√© Zero-Trust :** Int√®gre la v√©rification JWT, le Rate Limiting (limitation de taux), une politique CORS stricte et le masquage des informations PII.
*   **Endpoint `/api/ml_predict` :** Align√© *au pixel pr√®s* avec le code de la Phase I pour √©liminer tout *Training-Serving Skew*.

### 2. Interface Utilisateur Conversationnelle (Streamlit) `app.py`
Le portail dynamique :
*   **100% Agents IA :** Plus d'inputs fastidieux ni d'onglets de donn√©es. L'utilisateur dialogue directement par texte ou en vocal. L'Agent IA s'occupe de l'extraction de param√®tres en t√¢che de fond.
*   **Reconnaissance Vocale (Whisper/Google STT) :** Un intercepteur de vocabulaire sp√©cifique au march√© local (ex: il corrige automatiquement "kahraba" en "karhba").

### 3. Conteneurisation (Docker)
L'application enti√®re est "Dockeris√©e". Le `docker-compose.yml` d√©ploie les deux services au sein d'un r√©seau interne (`insat_olea_network`) isol√© de l'ext√©rieur.

---

## üöÄ Comment Lancer l'Application (En 1 Commande)

Gr√¢ce √† Docker Compose, le d√©ploiement sur votre machine est imm√©diat.

### Pr√©requis
- [Docker](https://www.docker.com/) et [Docker Compose](https://docs.docker.com/compose/)
- Cl√© API NVIDIA valide pour le LLaMA-70B

### √âtapes d'installation

1. **Configurer l'environnement :**
   Cr√©ez un fichier `.env` √† la racine de ce dossier avec votre cl√© API :
   ```bash
   NVIDIA_API_KEY="votre_cle_api_nvidia_ici"
   ```

2. **Lancer les conteneurs :**
   ```bash
   docker-compose up --build -d
   ```

3. **Acc√©der √† l'application :**
   - **Interface Utilisateur (Streamlit) :** Rendez-vous sur [http://localhost:8501](http://localhost:8501)
   - **Documentation de l'API (Swagger) :** Rendez-vous sur [http://localhost:8000/docs](http://localhost:8000/docs)

---

## üõ°Ô∏è Focus S√©curit√© (Zero-Trust)
Notre conception int√®gre des m√©canismes dignes d'une architecture d'entreprise en production :
1. **Rate Limiting :** Bloque automatiquement les attaques (DDoS/Spam) bas√©es sur l'identit√© (User ID masqu√©).
2. **PII Masking :** Les identifiants sensibles n'apparaissent jamais en clair dans l'immouvable `audit.log`.
3. **Internal Networking :** Le backend n'expose aucun port au public s'il est d√©ploy√© derri√®re une gateway cloud, le frontend communique avec lui via le r√©seau Docker priv√©.

---

## üì¶ Reproduction de la Phase I
Le projet assure une portabilit√© totale de la comp√©tition de base. `solution.py` et `model.pkl` sont la fondation de l'application. Le `requirements.txt` de base est rest√© d√©lib√©r√©ment vide pour respecter les conditions strictes de l'ar√®ne EvalDA, tandis que les microservices Phase II provisionnent leurs propres d√©pendances via `requirements_api.txt` et `requirements_ui.txt`.